From: Nicole Chalhoub <n-chalhoub@ti.com>
Date: Wed, 27 Apr 2011 12:50:40 +0000 (+0200)
Subject: [OMAP4] Update hotplug governor policy
X-Git-Tag: android-2.6.35-2.3-omap4.13.1-Beta~30
X-Git-Url: http://git.omapzoom.org/?p=kernel%2Fomap.git;a=commitdiff_plain;h=91829d4da9dec5cb1aacd482004e6ba4bd610e07

[OMAP4] Update hotplug governor policy

When 2 CPUs are plugged on, use the max load to decide
increasing/decreasing the frequency. Keep on using the average
load in the CPU hotplug policy.
Update the frequency decrease policy as done by the ondemand
governor: don't wait to cross the down_threshold in order to
decrease the frequency, go directly to the lowest frequency which
can support the current CPU utilization by keeping 30% of idle in
order to not cross the up_threshold.

The down_threshold is changed to 30 instead of 20 based on experiments.
but it is not considered on T1 OMAP4 samsung platform. 

This patch shows a good performance and power impact on the different
UCs of the OMAP4 platform:
- Divide by 2 the time of the sunspider web browsing benchmark.
- Gain of ~15mW on AV record use case.

Index: kernel/drivers/cpufreq/cpufreq_hotplug.c
===================================================================
--- kernel.orig/drivers/cpufreq/cpufreq_hotplug.c	2011-06-01 21:09:06.978347542 +0900
+++ kernel/drivers/cpufreq/cpufreq_hotplug.c	2011-06-01 21:23:35.478389747 +0900
@@ -36,11 +36,12 @@
 /* greater than 80% avg load across online CPUs increases frequency */
 #define DEFAULT_UP_FREQ_MIN_LOAD			(80)
 
+/* Keep 10% of idle under the up threshold when decreasing the frequency */
+#define DEFAULT_FREQ_DOWN_DIFFERENTIAL			(10)
+
 /* less than 20% avg load across online CPUs decreases frequency */
 #define DEFAULT_DOWN_FREQ_MAX_LOAD			(20)
 
-#define DEFAULT_CORE1_OFF_MAX_LOAD			(20)
-
 /* default sampling period (uSec) is bogus; 10x ondemand's default for x86 */
 #define DEFAULT_SAMPLING_PERIOD				(100000)
 
@@ -112,8 +113,8 @@
 static struct dbs_tuners {
 	unsigned int sampling_rate;
 	unsigned int up_threshold;
+	unsigned int down_differential;
 	unsigned int down_threshold;
-	unsigned int core1_off_threshold;
 	unsigned int hotplug_in_sampling_periods;
 	unsigned int hotplug_out_sampling_periods;
 	unsigned int hotplug_load_index;
@@ -124,8 +125,8 @@
 } dbs_tuners_ins = {
 	.sampling_rate =		DEFAULT_SAMPLING_PERIOD,
 	.up_threshold =			DEFAULT_UP_FREQ_MIN_LOAD,
+	.down_differential =            DEFAULT_FREQ_DOWN_DIFFERENTIAL,
 	.down_threshold =		DEFAULT_DOWN_FREQ_MAX_LOAD,
-	.core1_off_threshold = DEFAULT_CORE1_OFF_MAX_LOAD,
 	.hotplug_in_sampling_periods =	DEFAULT_HOTPLUG_IN_SAMPLING_PERIODS,
 	.hotplug_out_sampling_periods =	DEFAULT_HOTPLUG_OUT_SAMPLING_PERIODS,
 	.hotplug_load_index =		0,
@@ -173,8 +174,8 @@
 }
 show_one(sampling_rate, sampling_rate);
 show_one(up_threshold, up_threshold);
+show_one(down_differential, down_differential);
 show_one(down_threshold, down_threshold);
-show_one(core1_off_threshold, core1_off_threshold);
 show_one(hotplug_in_sampling_periods, hotplug_in_sampling_periods);
 show_one(hotplug_out_sampling_periods, hotplug_out_sampling_periods);
 show_one(ignore_nice_load, ignore_nice);
@@ -228,36 +229,36 @@
 	return count;
 }
 
-static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
 				  const char *buf, size_t count)
 {
 	unsigned int input;
 	int ret;
 	ret = sscanf(buf, "%u", &input);
 
-	if (ret != 1 || input >= dbs_tuners_ins.up_threshold) {
+	if (ret != 1 || input >= dbs_tuners_ins.up_threshold)
 		return -EINVAL;
-	}
 
 	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.down_threshold = input;
+	dbs_tuners_ins.down_differential = input;
 	mutex_unlock(&dbs_mutex);
 
 	return count;
 }
-static ssize_t store_core1_off_threshold(struct kobject *a, struct attribute *b,
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
 				  const char *buf, size_t count)
 {
 	unsigned int input;
 	int ret;
 	ret = sscanf(buf, "%u", &input);
 
-	if (ret != 1 || input >= dbs_tuners_ins.down_threshold) {
+	if (ret != 1 || input >= dbs_tuners_ins.up_threshold) {
 		return -EINVAL;
 	}
 
 	mutex_lock(&dbs_mutex);
-	dbs_tuners_ins.core1_off_threshold = input;
+	dbs_tuners_ins.down_threshold = input;
 	mutex_unlock(&dbs_mutex);
 
 	return count;
@@ -417,8 +418,8 @@
 
 define_one_global_rw(sampling_rate);
 define_one_global_rw(up_threshold);
+define_one_global_rw(down_differential);
 define_one_global_rw(down_threshold);
-define_one_global_rw(core1_off_threshold);
 define_one_global_rw(hotplug_in_sampling_periods);
 define_one_global_rw(hotplug_out_sampling_periods);
 define_one_global_rw(ignore_nice_load);
@@ -428,8 +429,8 @@
 static struct attribute *dbs_attributes[] = {
 	&sampling_rate.attr,
 	&up_threshold.attr,
+	&down_differential.attr,
 	&down_threshold.attr,
-	&core1_off_threshold.attr,
 	&hotplug_in_sampling_periods.attr,
 	&hotplug_out_sampling_periods.attr,
 	&ignore_nice_load.attr,
@@ -449,8 +450,10 @@
 {
 	/* combined load of all enabled CPUs */
 	unsigned int total_load = 0;
-	/* single largest CPU load */
+	/* single largest CPU load percentage*/
 	unsigned int max_load = 0;
+	/* largest CPU load in terms of frequency */
+	unsigned int max_load_freq = 0;
 	/* average load across all enabled CPUs */
 	unsigned int avg_load = 0;
 	/* average load across multiple sampling periods for hotplug events */
@@ -460,7 +463,6 @@
 	unsigned int periods;
 
 	struct cpufreq_policy *policy;
-	unsigned int index = 0;
 	unsigned int i, j;
 	struct device *mpu_dev = omap2_get_mpuss_device();
 
@@ -505,6 +507,9 @@
 			max_load = load;
 	}
 
+	/* use the max load in the OPP freq change policy */
+	max_load_freq = max_load * policy->cur;
+
 	/* calculate the average load across all related CPUs */
 	avg_load = total_load / num_online_cpus();
 
@@ -547,7 +552,7 @@
 	if (++dbs_tuners_ins.hotplug_load_index == periods)
 		dbs_tuners_ins.hotplug_load_index = 0;
 
-	/* check for frequency increase */
+	/* check if auxiliary CPU is needed based on avg_load */
 	if (avg_load > dbs_tuners_ins.up_threshold) {
 		/* should we enable auxillary CPUs? */
 		if (num_online_cpus() < 2 && hotplug_in_avg_load >
@@ -558,7 +563,10 @@
 			dpll_cascading_blocker_hold(mpu_dev);
 			goto out;
 		}
+	}
 
+	/* check for frequency increase based on max_load */
+	if (max_load > dbs_tuners_ins.up_threshold) {
 		/* increase to highest frequency supported */
 		if (policy->cur < policy->max)
 			__cpufreq_driver_target(policy, policy->max,
@@ -572,14 +580,14 @@
 		/* are we at the minimum frequency already? */
 		if (policy->cur <= policy->min) {
 			/* should we disable auxillary CPUs? */
-#if 0
+#if 1
 			if (num_online_cpus() > 1 && hotplug_out_avg_load <
 					dbs_tuners_ins.down_threshold) {
 #else
 			if (num_online_cpus() > 1 && hotplug_out_avg_load <
 				dbs_tuners_ins.core1_off_threshold) {
-				printk("hotplug out avg load = %d", hotplug_out_avg_load);
 #endif
+				printk("hotplug out avg load = %d", hotplug_out_avg_load);
 
 				omap4_duty_policy_lock();
 				cpu_down(1);
@@ -588,18 +596,25 @@
 			}
 			goto out;
 		}
+	}
 
-		/* bump down to the next lowest frequency in the table */
-		if (cpufreq_frequency_table_next_lowest(policy,
-					this_dbs_info->freq_table, &index)) {
-			pr_err("%s: failed to get next lowest frequency\n",
-					__func__);
-			goto out;
-		}
+	/*
+	 * go down to the lowest frequency which can sustain the load by
+	 * keeping 20% of idle in order to not cross the up_threshold
+	 */
+	if ((max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+	     policy->cur) && (policy->cur > policy->min)) {
+		unsigned int freq_next;
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold -
+				 dbs_tuners_ins.down_differential);
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
 
-		__cpufreq_driver_target(policy,
-				this_dbs_info->freq_table[index].frequency,
-				CPUFREQ_RELATION_L);
+		 __cpufreq_driver_target(policy, freq_next,
+					 CPUFREQ_RELATION_L);
 	}
 out:
 	mutex_unlock(&dbs_mutex);
